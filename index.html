<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RhythmGuassian - Remote Physiological Measurement</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.8/dist/chart.umd.min.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    colors: {
                        primary: '#2563EB', // 科研蓝
                        secondary: '#059669', // 生理信号绿
                        dark: '#1E293B',
                        light: '#F8FAFC'
                    },
                    fontFamily: {
                        inter: ['Inter', 'sans-serif'],
                    },
                }
            }
        }
    </script>
    
    <style type="text/tailwindcss">
        @layer utilities {
            .content-auto {
                content-visibility: auto;
            }
            .section-fade {
                opacity: 0;
                transform: translateY(20px);
                transition: opacity 0.6s ease, transform 0.6s ease;
            }
            .section-visible {
                opacity: 1;
                transform: translateY(0);
            }
            .text-balance {
                text-wrap: balance;
            }
        }
    </style>
</head>

<body class="font-inter bg-white text-dark">
    <!-- 导航栏 -->
    <header class="fixed w-full bg-white/95 backdrop-blur-sm shadow-sm z-50 transition-all duration-300">
        <div class="container mx-auto px-4 py-3 flex justify-between items-center">
            <a href="#" class="flex items-center gap-2">
                <div class="w-10 h-10 rounded-lg bg-primary/10 flex items-center justify-center">
                    <i class="fa fa-heartbeat text-primary text-xl"></i>
                </div>
                <span class="text-xl font-bold">RhythmGuassian</span>
            </a>
            
            <!-- 桌面导航 -->
            <nav class="hidden md:flex items-center gap-8">
                <a href="#overview" class="hover:text-primary transition-colors">Overview</a>
                <a href="#framework" class="hover:text-primary transition-colors">Framework</a>
                <a href="#method" class="hover:text-primary transition-colors">Method</a>
                <a href="#authors" class="hover:text-primary transition-colors">Authors</a>
                <a href="https://github.com/LuPaoPao/RhythmGuassian" target="_blank" class="px-5 py-2 bg-primary text-white rounded-lg hover:bg-primary/90 transition-colors flex items-center gap-2">
                    <i class="fa fa-github"></i> Code
                </a>
            </nav>
            
            <!-- 移动端菜单按钮 -->
            <button id="menuToggle" class="md:hidden text-xl">
                <i class="fa fa-bars"></i>
            </button>
        </div>
        
        <!-- 移动端菜单 -->
        <div id="mobileMenu" class="hidden md:hidden bg-white border-t">
            <div class="container mx-auto px-4 py-4 flex flex-col gap-4">
                <a href="#overview" class="py-2 hover:text-primary transition-colors">Overview</a>
                <a href="#framework" class="py-2 hover:text-primary transition-colors">Framework</a>
                <a href="#method" class="py-2 hover:text-primary transition-colors">Method</a>
                <a href="#authors" class="py-2 hover:text-primary transition-colors">Authors</a>
                <a href="https://github.com/LuPaoPao/RhythmGuassian" target="_blank" class="py-2 bg-primary text-white rounded-lg text-center hover:bg-primary/90 transition-colors">
                    <i class="fa fa-github mr-2"></i> Code
                </a>
            </div>
        </div>
    </header>

    <main>
        <!-- 英雄区域 -->
        <section class="pt-32 pb-20 md:pt-40 md:pb-28 bg-gradient-to-b from-light to-white">
            <div class="container mx-auto px-4">
                <div class="flex flex-col md:flex-row items-center gap-12">
                    <div class="md:w-1/2 space-y-6">
                        <div class="inline-block px-4 py-1.5 bg-primary/10 text-primary rounded-full text-sm font-medium">
                            <i class="fa fa-flask mr-2"></i> Remote Physiological Measurement
                        </div>
                        <h1 class="text-[clamp(1.8rem,4vw,3rem)] font-bold leading-tight">
                            RhythmGuassian: Repurposing Generalizable Gaussian Model For Remote Physiological Measurement
                        </h1>
                        <p class="text-lg text-gray-600 max-w-lg">
                            A novel framework using 4D Gaussian representations to address motion entanglement in remote Photoplethysmography (rPPG) signal extraction.
                        </p>
                        <div class="flex flex-wrap gap-4 pt-4">
                            <a href="https://github.com/LuPaoPao/RhythmGuassian" target="_blank" class="px-6 py-3 bg-primary text-white rounded-lg hover:bg-primary/90 transition-all shadow-lg shadow-primary/20">
                                <i class="fa fa-github mr-2"></i> GitHub Repository
                            </a>
                            <a href="#framework" class="px-6 py-3 bg-white text-dark border border-gray-300 rounded-lg hover:bg-gray-50 transition-all">
                                <i class="fa fa-sitemap mr-2"></i> View Framework
                            </a>
                        </div>
                    </div>
                    <div class="md:w-1/2 relative">
                        <div class="rounded-xl overflow-hidden shadow-2xl">
                            <!-- 建议替换为实际框架图 -->
                            <img src="figure/framework.pdf" alt="RhythmGuassian Framework Overview" class="w-full h-auto">
                            <div class="absolute inset-0 bg-gradient-to-t from-black/60 to-transparent flex items-end">
                                <div class="p-6 text-white">
                                    <p class="font-medium">Proposed Framework</p>
                                    <p class="text-sm text-gray-200">4D Gaussian-based rPPG extraction pipeline</p>
                                </div>
                            </div>
                        </div>
                        <!-- 装饰元素 -->
                        <div class="absolute -bottom-6 -left-6 w-24 h-24 bg-secondary/20 rounded-full blur-xl"></div>
                        <div class="absolute -top-6 -right-6 w-32 h-32 bg-primary/20 rounded-full blur-xl"></div>
                    </div>
                </div>
            </div>
        </section>

        <!-- 项目概述 -->
        <section id="overview" class="py-20 section-fade">
            <div class="container mx-auto px-4">
                <div class="max-w-3xl mx-auto text-center mb-16">
                    <h2 class="text-3xl md:text-4xl font-bold mb-6">Abstract</h2>
                    <div class="bg-light p-8 rounded-xl text-left shadow-sm border border-gray-100">
                        <p class="text-gray-700 leading-relaxed">
                            Remote Photoplethysmography (rPPG) enables non-contact extraction of physiological signals, providing significant advantages in medical monitoring, emotion recognition, and face anti-spoofing. However, the extraction of reliable rPPG signals is hindered by motion variations in real-world environments, leading to entanglement issues.
                        </p>
                        <p class="text-gray-700 leading-relaxed mt-4">
                            To address this challenge, we employ the Generalizable Gaussian Model (GGM) to disentangle geometry and chroma components with 4D Gaussian representations. Employing the GGM for robust rPPG estimation is non-trivial:
                        </p>
                        <ul class="list-disc pl-6 text-gray-700 leading-relaxed mt-2 space-y-2">
                            <li>Firstly, dataset lacks camera parameters, preventing video rendering from 4D Gaussian. We propose a "4D virtual camera" to construct extra parameters describing view and motion changes, enabling video rendering with fixed virtual camera parameters.</li>
                            <li>Further, chroma components are not explicitly decoupled in 4D Gaussian representations. We design Explicit Motion Modeling (EMM) to decouple motion variation in an unsupervised manner, and Explicit Chroma Modeling (ECM) to decouple specular, physiological, and noise signals.</li>
                        </ul>
                        <p class="text-gray-700 leading-relaxed mt-4">
                            To validate our approach, we expand existing rPPG datasets to include various motion and illumination interference scenarios, demonstrating effectiveness in real-world settings. Code is available at <a href="https://github.com/LuPaoPao/RhythmGuassian" class="text-primary underline" target="_blank">https://github.com/LuPaoPao/RhythmGuassian</a>.
                        </p>
                    </div>
                </div>
            </div>
        </section>

        <!-- 框架图 -->
        <section id="framework" class="py-20 bg-white section-fade">
            <div class="container mx-auto px-4">
                <div class="max-w-3xl mx-auto text-center mb-16">
                    <h2 class="text-3xl md:text-4xl font-bold mb-6">Framework Overview</h2>
                    <p class="text-lg text-gray-600">
                        The RhythmGuassian framework consists of four core components working in tandem to enable robust rPPG extraction.
                    </p>
                </div>
                
                <div class="bg-light p-6 md:p-10 rounded-xl shadow-lg">
                    <div class="flex justify-center mb-8">
                        <img src="figure/framework.pdf" alt="RhythmGuassian Framework" class="w-full max-w-5xl rounded-lg">
                    </div>
                    <div class="max-w-5xl mx-auto text-gray-700">
                        <p class="text-sm text-gray-500 italic text-center mb-6">
                            Figure 1: An overview of the proposed method. (a) Generalized Gaussian Model; (b) Explicit Motion Modeling (EMM); (c) 4D Virtual Camera; (d) Explicit Chroma Modeling (ECM).
                        </p>
                        
                        <div class="grid md:grid-cols-2 gap-8 mt-10">
                            <div class="bg-white p-6 rounded-lg shadow-sm">
                                <h4 class="font-bold text-lg mb-3 text-primary">Generalized Gaussian Model</h4>
                                <p class="text-gray-600 text-sm">
                                    The STMap, derived from the input video, is processed through an encoder and fed into the physiological decoder to predict the physiological signals. Another path feeds into the Gaussian Adaptor to obtain 4D Gaussian Map.
                                </p>
                            </div>
                            <div class="bg-white p-6 rounded-lg shadow-sm">
                                <h4 class="font-bold text-lg mb-3 text-primary">Explicit Motion Modeling (EMM)</h4>
                                <p class="text-gray-600 text-sm">
                                    Achieves spatial motion compensation in 4D Gaussian space by applying Δh (horizontal displacement), Δw (width variation), and Δs (scale adjustment) parameters to the Gaussian splatting set P, ultimately generating the motion-modified point set P̂.
                                </p>
                            </div>
                            <div class="bg-white p-6 rounded-lg shadow-sm">
                                <h4 class="font-bold text-lg mb-3 text-primary">4D Virtual Camera</h4>
                                <p class="text-gray-600 text-sm">
                                    Uses the motion-modified point set to render videos with tri-stream chrominance components (Vs, Vd, and Vn), while optimizing the model through pixel-wise reconstruction loss between rendered and input videos.
                                </p>
                            </div>
                            <div class="bg-white p-6 rounded-lg shadow-sm">
                                <h4 class="font-bold text-lg mb-3 text-primary">Explicit Chroma Modeling (ECM)</h4>
                                <p class="text-gray-600 text-sm">
                                    Spatiotemporal Consistency Loss enforces inter-frame stability of Vd across facial regions. Motion-Aware Chroma Consistency Loss ℒₘ synchronizes pose parameters [Δh, Δw, Δs] ∈ ℝ^(5×N×T) with noise-associated chroma component Vdn ∈ ℝ^(3×1×T).
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- 核心方法 -->
        <section id="method" class="py-20 bg-light section-fade">
            <div class="container mx-auto px-4">
                <div class="max-w-3xl mx-auto text-center mb-16">
                    <h2 class="text-3xl md:text-4xl font-bold mb-6">Key Innovations</h2>
                    <p class="text-lg text-gray-600">
                        Our method introduces three critical innovations to address the challenges of rPPG extraction under motion variations.
                    </p>
                </div>
                
                <div class="grid md:grid-cols-3 gap-8">
                    <div class="bg-white p-8 rounded-xl shadow-lg hover:shadow-xl transition-shadow border border-gray-100">
                        <div class="w-14 h-14 bg-primary/10 rounded-lg flex items-center justify-center mb-6">
                            <i class="fa fa-camera text-primary text-2xl"></i>
                        </div>
                        <h3 class="text-xl font-bold mb-3">4D Virtual Camera</h3>
                        <p class="text-gray-600 mb-4">
                            Constructs extra Gaussian parameters to describe view and motion changes, enabling video rendering with fixed virtual camera parameters even when no real camera parameters are available.
                        </p>
                        <p class="text-gray-500 text-sm">
                            Solves the critical problem of video reconstruction from 4D Gaussian representations without camera metadata.
                        </p>
                    </div>
                    
                    <div class="bg-white p-8 rounded-xl shadow-lg hover:shadow-xl transition-shadow border border-gray-100">
                        <div class="w-14 h-14 bg-secondary/10 rounded-lg flex items-center justify-center mb-6">
                            <i class="fa fa-arrows text-secondary text-2xl"></i>
                        </div>
                        <h3 class="text-xl font-bold mb-3">Explicit Motion Modeling</h3>
                        <p class="text-gray-600 mb-4">
                            Applies horizontal displacement (Δh), width variation (Δw), and scale adjustment (Δs) to Gaussian splatting sets for unsupervised motion decoupling.
                        </p>
                        <p class="text-gray-500 text-sm">
                            Achieves spatial motion compensation directly in 4D Gaussian space to reduce motion artifacts.
                        </p>
                    </div>
                    
                    <div class="bg-white p-8 rounded-xl shadow-lg hover:shadow-xl transition-shadow border border-gray-100">
                        <div class="w-14 h-14 bg-primary/10 rounded-lg flex items-center justify-center mb-6">
                            <i class="fa fa-tint text-primary text-2xl"></i>
                        </div>
                        <h3 class="text-xl font-bold mb-3">Explicit Chroma Modeling</h3>
                        <p class="text-gray-600 mb-4">
                            Decouples specular (Vs), physiological (Vd), and noise (Vn) signals through specialized loss functions and consistency constraints.
                        </p>
                        <p class="text-gray-500 text-sm">
                            Preserves physiological signal consistency while suppressing noise and specular reflections.
                        </p>
                    </div>
                </div>
            </div>
        </section>

       
